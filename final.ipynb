{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYyigANtDk/LikIqH20NRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Edward-The-Great/DATAset-for-my-finals/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBx66wJe2LTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd8d0c2-ae9d-4dea-c70e-5adc0123ff60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                results for KNN  \n",
            "Train = Accuracy=0.844, Precision=0.814, Recall=0.720, F1=0.764\n",
            "Test  = Accuracy=0.844, Precision=0.814, Recall=0.720, F1=0.764\n",
            "\n",
            "            results for Naive Bayes \n",
            "Train = Accuracy=0.767, Precision=0.693, Recall=0.608, F1=0.648\n",
            "Test  = Accuracy=0.767, Precision=0.693, Recall=0.608, F1=0.648\n",
            "\n",
            "           results for Decision Tree \n",
            "Train = Accuracy =0.790, Precision=0.711, Recall=0.677, F1=0.694\n",
            "Test  = Accuracy =0.790, Precision=0.711, Recall=0.677, F1=0.694\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# 1) first of.. LOAD AND SPLIT DATA\n",
        "def load_data_from_url(csv_url):\n",
        "\n",
        "    df = pd.read_csv(csv_url)\n",
        "    return df\n",
        "    #1.2 now i have got the data frame loaded.\n",
        "    #1.3  actual url provided in section #10\n",
        "\n",
        "#2) now split to training and testing sets\n",
        "def split_dataset(df, train_ratio = 0.7):\n",
        "\n",
        "     n_total = len(df)\n",
        "     split_index = int(train_ratio * n_total)\n",
        "     df_train = df.iloc [:split_index].copy()\n",
        "     df_test = df.iloc [:split_index:].copy()\n",
        "     return df_train, df_test\n",
        "     #2.1 so now we got splitted data frame: and parameters!\n",
        "\n",
        "\n",
        "#3) start KNN classifier: use the parameter above\n",
        "def KNN(df_train):\n",
        "\n",
        "     #3.1 creating dictionary out of data\n",
        "      model_KNN = {\n",
        "     #3.2 i need a copy not to mess it up\n",
        "                     'train_data': df_train.copy()\n",
        "      }\n",
        "      return model_KNN\n",
        "\n",
        "\n",
        "\n",
        "#4 okalidi distance between two features\n",
        "def okalidi_merhak(a_feature, b_feature):\n",
        "    #4.1 initialize a digit variable\n",
        "    sq_sum = 0.0\n",
        "\n",
        "    #4.2 iterate over the features\n",
        "    for i in range(len(a_feature)):\n",
        "\n",
        "        #4.3 compute differance\n",
        "        diff = a_feature[i] - b_feature[i]\n",
        "\n",
        "        #4.4 accumulate the Squares\n",
        "        sq_sum += diff * diff\n",
        "\n",
        "        #4.5 return square root\n",
        "    return math.sqrt(sq_sum)\n",
        "\n",
        "#5 KNN prediction function!!\n",
        "def predict_clas_KNN(new_line, model_KNN, k=3):\n",
        "\n",
        "  #5.1 access 'train_data' from model_KNN. This is the entire training set\n",
        "  train_df = model_KNN['train_data']\n",
        "  dist_labels = []\n",
        "\n",
        "  #5.2 iterate over rows\n",
        "  for _, row in train_df.iterrows():\n",
        "    train_feats = row [:-1].values\n",
        "    train_label = row['Outcome']\n",
        "\n",
        "    #5.3 collect (distance, label) pairs\n",
        "    dist = okalidi_merhak (new_line, train_feats)\n",
        "    dist_labels.append((dist, train_label))\n",
        "\n",
        "  #5.4 sort by distance ascending\n",
        "  dist_labels.sort(key=lambda x: x[0])\n",
        "\n",
        "  #5.5 get the top k labels\n",
        "  top_k = dist_labels[:k]\n",
        "\n",
        "  #5.5 majoroty vote\n",
        "  count_ones = sum(1 for (d, lab) in top_k if lab == 1)\n",
        "  count_zeros = k - count_ones\n",
        "  prediction = 1 if count_ones > count_zeros else 0\n",
        "\n",
        "  #5.6 returns prediction = 0 or 1\n",
        "  p1 = count_ones/ float(k)\n",
        "  p0 = 1- p1\n",
        "  return prediction, p1, p0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 6 NAIVE BAYES classifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#6.1 split df into classes\n",
        "def separate_by_class(df):\n",
        "    df_class0 = df[df['Outcome'] == 0].copy()\n",
        "    df_class1 = df[df['Outcome'] == 1].copy()\n",
        "    return df_class0, df_class1\n",
        "    # we got them classes\n",
        "\n",
        "#6.2 compute mean & variance (ddof=1) for 8 features\n",
        "def compute_stats_for_class(df_class):\n",
        "    features = df_class.iloc[:, :-1]\n",
        "    means = features.mean(axis=0).values\n",
        "    variances = features.var(axis=0, ddof=1).values\n",
        "    return means, variances\n",
        "\n",
        "\n",
        "\n",
        "# 7 Probability density function of a Normal distribution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#7.1 (mean,var) for a single future value x\n",
        "def gaausian_pdf(x, mean, var):\n",
        "\n",
        "    #7.2 to avoid zero variance\n",
        "    eps = 1e-4\n",
        "    if var < eps:\n",
        "       var = eps\n",
        "\n",
        "    # 7.3 standard deviation\n",
        "    std = math.sqrt(var)\n",
        "    exponent = math.exp(-(math.pow(x - mean, 2) / (2 * var)))\n",
        "    return (1.0 / (math.sqrt(2 * math.pi) * std)) * exponent\n",
        "\n",
        "# 8 compute the unnormalized posterior for one class\n",
        "def posterior_score(row_features, means, variances, prior):\n",
        "\n",
        "   #8.1  the prior probability for that class\n",
        "   score = prior\n",
        "   for i in range(len(row_features)):\n",
        "      x_i = row_features[i]\n",
        "      mu_i = means[i]\n",
        "      var_i = variances[i]\n",
        "\n",
        "      #8.2 multiply by the product of gusian PDF for each feature\n",
        "      score *= gaausian_pdf(x_i, mu_i, var_i)\n",
        "   return score\n",
        "\n",
        "# 9 training function\n",
        "def train_naive_bayes(df_train):\n",
        "\n",
        "    #9.1 splitting rows into df0 (Outcome=0) and df1 (Outcome=1)\n",
        "    df0, df1 = separate_by_class(df_train)\n",
        "    n_total = len(df_train)\n",
        "    n_c0 = len(df0)\n",
        "    n_c1 = len(df1)\n",
        "\n",
        "    #9.2 computing the prior probabilities\n",
        "    prior_c0 = n_c0 / float(n_total)\n",
        "    prior_c1 = n_c1 / float(n_total)\n",
        "\n",
        "    #9.3 calculating means/variances for each class\n",
        "    means0, vars0 = compute_stats_for_class(df0)\n",
        "    means1, vars1 = compute_stats_for_class(df1)\n",
        "\n",
        "    #9.4 now i need a dictionary storing all these values\n",
        "    model_naive_bayes = {\n",
        "        'means0': means0, 'vars0': vars0, 'prior0': prior_c0,\n",
        "        'means1': means1, 'vars1': vars1, 'prior1': prior_c1\n",
        "    }\n",
        "    #9.5 return the dic\n",
        "    return model_naive_bayes\n",
        "\n",
        "#10 predict class for a new data row\n",
        "def predict_clas_naive_bayes(new_line, model_naive_bayes):\n",
        "\n",
        "    #10.1  compute the Lo normali posteriors for class 0\n",
        "    score0 = posterior_score(new_line,\n",
        "                             model_naive_bayes['means0'],\n",
        "                             model_naive_bayes['vars0'],\n",
        "                             model_naive_bayes['prior0'])\n",
        "\n",
        "    #10.2 Compute the Lo normali posteriors for class 1\n",
        "    score1 = posterior_score(new_line,\n",
        "                             model_naive_bayes['means1'],\n",
        "                             model_naive_bayes['vars1'],\n",
        "                             model_naive_bayes['prior1'])\n",
        "    #10.3 ssssssssssssssssssss\n",
        "    total = score0 + score1\n",
        "    if total < 1e-15:\n",
        "      p0 = p1 = 0.5\n",
        "    else:\n",
        "      p0 = score0 / total\n",
        "      p1 = score1 / total\n",
        "\n",
        "    # 10.4 ssssssssssssssssss\n",
        "    prediction = 0 if score0 > score1 else 1\n",
        "    return prediction, p0, p1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 11 decision tree CLASSifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#11.1 compute shannon entropy, for a binary labels (0 or 1)\n",
        "def entropy(samples):\n",
        "  n_total = len(samples)\n",
        "  if n_total == 0:\n",
        "    return 0\n",
        "  count_ones = sum(samples)\n",
        "  p1 = count_ones / float(n_total)\n",
        "  if p1 == 0 or p1==1:\n",
        "    return 0\n",
        "  p0 = 1 - p1\n",
        "  return - (p0 * math.log2(p0) + p1 * math.log2(p1))\n",
        "\n",
        "#11.2 find the best feature (and threshold = mean)\n",
        "#  to split the data in a simple ID3-like approach\n",
        "def best_split(df):\n",
        "    base_entropy = entropy(df['Outcome'].values)\n",
        "    n_features = df.shape[1] - 1\n",
        "    best_gain = 0.0\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "\n",
        "    # 11.3 xxxxxxxxxxx\n",
        "    for feat_index in range(n_features):\n",
        "        #11.4 pick treshold = mean of that column\n",
        "        column_values = df.iloc[:, feat_index].values\n",
        "        threshold = np.mean(column_values)\n",
        "\n",
        "        #11.5 split df into left and right by that treshold\n",
        "        left_df = df[df.iloc[:, feat_index] <= threshold]\n",
        "        right_df = df[df.iloc[:, feat_index] > threshold]\n",
        "\n",
        "        # 11.6\n",
        "        left_entropy = entropy(left_df['Outcome'].values)\n",
        "        right_entropy = entropy(right_df['Outcome'].values)\n",
        "\n",
        "        # 11.7\n",
        "        w_left = len(left_df) / float(len(df))\n",
        "        w_right = 1 - w_left\n",
        "\n",
        "        # 11.8\n",
        "        new_entopy = w_left * left_entropy + w_right * right_entropy\n",
        "        info_gain = base_entropy - new_entopy\n",
        "\n",
        "        # 11.9\n",
        "        if info_gain > best_gain:\n",
        "            best_gain = info_gain\n",
        "            best_feature = feat_index\n",
        "            best_threshold = threshold\n",
        "    #  11.10\n",
        "    if best_feature is None or best_gain < 1e-9:\n",
        "         return None\n",
        "    return (best_feature, best_threshold)\n",
        "\n",
        "#12 Recursively build a decision tree\n",
        "def build_tree(df, depth=0, max_depth=5):\n",
        "\n",
        "    # 12.1\n",
        "    outcomes = df['Outcome'].values\n",
        "    n_ones = sum(outcomes)\n",
        "    n_zeros = len(df) - n_ones\n",
        "\n",
        "    #12.2 If pure or empty or max_depth reached => leaf\n",
        "    if len(df) == 0 or n_ones == 0 or n_zeros == 0 or depth >= max_depth:\n",
        "\n",
        "        #12.3 majority class\n",
        "        maj = 1 if n_ones >= n_zeros else 0\n",
        "        return {'leaf': True, 'outcome': maj}\n",
        "    # 12.4\n",
        "    split = best_split(df)\n",
        "    if split is None:\n",
        "\n",
        "        #12.5 no good split = leaf\n",
        "        maj = 1 if n_ones >= n_zeros else 0\n",
        "        return {'leaf': True, 'outcome': maj}\n",
        "    feat, thr = split\n",
        "\n",
        "    #12.6 build subtrees\n",
        "    left_df = df[df.iloc[:, feat] <= thr]\n",
        "    right_df = df[df.iloc[:, feat] > thr]\n",
        "\n",
        "    # 12.7\n",
        "    left_subtree = build_tree(left_df, depth+1, max_depth)\n",
        "    right_subtree = build_tree(right_df, depth+1, max_depth)\n",
        "\n",
        "    # 12.8\n",
        "    return {\n",
        "        'leaf': False,\n",
        "        'feature': feat,\n",
        "        'threshold': thr,\n",
        "        'left': left_subtree,\n",
        "        'right': right_subtree\n",
        "    }\n",
        "\n",
        "#13 build a tree from training df.\n",
        "def train_decision_tree(df_train, max_depth=5):\n",
        "\n",
        "    #13.1 Returns 'model_dt' which is the root node of the tree.\n",
        "    root = build_tree(df_train, depth=0, max_depth=max_depth)\n",
        "    model_dt = {\n",
        "        'root': root\n",
        "    }\n",
        "    return model_dt\n",
        "\n",
        "#14  Predict a single row's outcome by traversing the tree node.\n",
        "def predict_one_dt(row_features, node):\n",
        "    # 14.1\n",
        "    if node['leaf']:\n",
        "        return node['outcome']  # 0 or 1\n",
        "    # 14.2\n",
        "    feat = node['feature']\n",
        "    thr = node['threshold']\n",
        "    val = row_features[feat]\n",
        "\n",
        "    # 14.3\n",
        "    if val <= thr:\n",
        "        return predict_one_dt(row_features, node['left'])\n",
        "    else:\n",
        "        return predict_one_dt(row_features, node['right'])\n",
        "\n",
        "#15 Predict for a single row. We'll only return label, no probability here.\n",
        "def predict_class_dt(new_row_features, model_dt):\n",
        "\n",
        "    # 15.1\n",
        "    root = model_dt['root']\n",
        "    pred = predict_one_dt(new_row_features, root)\n",
        "    return pred\n",
        "\n",
        "\n",
        "#16  EVALUATION (ACCURACY, PRECISION, RECALL, F1)\n",
        "def evaluate_knn(df, model_knn, k=3):\n",
        "\n",
        "    # 16.1\n",
        "    TP = FP = TN = FN = 0\n",
        "    for _, row in df.iterrows():\n",
        "\n",
        "        # 16.2\n",
        "        feats = row[:-1].values\n",
        "        true_label = row['Outcome']\n",
        "\n",
        "        # 16.3\n",
        "        pred_label, _, _ = predict_clas_KNN(feats, model_knn, k)\n",
        "\n",
        "\n",
        "        if pred_label == 1 and true_label == 1:\n",
        "            TP += 1\n",
        "        elif pred_label == 1 and true_label == 0:\n",
        "            FP += 1\n",
        "        elif pred_label == 0 and true_label == 0:\n",
        "            TN += 1\n",
        "        elif pred_label == 0 and true_label == 1:\n",
        "\n",
        "            # 16.4\n",
        "            FN += 1\n",
        "\n",
        "    # 16.5\n",
        "    total = TP+FP+TN+FN\n",
        "    accuracy = (TP+TN)/total if total>0 else 0\n",
        "    precision = TP/(TP+FP) if (TP+FP)>0 else 0\n",
        "    recall = TP/(TP+FN) if (TP+FN)>0 else 0\n",
        "    f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "#17 Evaluate Naive Bayes = returns (accuracy, precision, recall, f1)\n",
        "def evaluate_nb(df, model_nb):\n",
        "\n",
        "    # 17.1\n",
        "    TP = FP = TN = FN = 0\n",
        "\n",
        "    # 17.2\n",
        "    for _, row in df.iterrows():\n",
        "\n",
        "        # 17.3\n",
        "        feats = row[:-1].values\n",
        "        true_label = row['Outcome']\n",
        "\n",
        "        # 17.4\n",
        "        pred_label, _, _ = predict_clas_naive_bayes(feats, model_nb)\n",
        "\n",
        "        # 17.5\n",
        "        if pred_label == 1 and true_label == 1:\n",
        "            TP += 1\n",
        "        elif pred_label == 1 and true_label == 0:\n",
        "            FP += 1\n",
        "        elif pred_label == 0 and true_label == 0:\n",
        "            TN += 1\n",
        "        elif pred_label == 0 and true_label == 1:\n",
        "\n",
        "            # 17.6\n",
        "            FN += 1\n",
        "\n",
        "    # 17.7\n",
        "    total = TP+FP+TN+FN\n",
        "\n",
        "    accuracy = (TP+TN)/total if total>0 else 0\n",
        "    precision = TP/(TP+FP) if (TP+FP)>0 else 0\n",
        "    recall = TP/(TP+FN) if (TP+FN)>0 else 0\n",
        "    f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "#18 Evaluate Decision Tree = returns (accuracy, precision, recall, f1)\n",
        "def evaluate_dt(df, model_dt):\n",
        "\n",
        "    # 18.1\n",
        "    TP = FP = TN = FN = 0\n",
        "\n",
        "    # 18.2\n",
        "    for _, row in df.iterrows():\n",
        "\n",
        "        # 18.3\n",
        "        feats = row[:-1].values\n",
        "        true_label = row['Outcome']\n",
        "\n",
        "        # 18.4\n",
        "        pred_label = predict_class_dt(feats, model_dt)\n",
        "\n",
        "        # 18.5\n",
        "        if pred_label == 1 and true_label == 1:\n",
        "            TP += 1\n",
        "        elif pred_label == 1 and true_label == 0:\n",
        "            FP += 1\n",
        "        elif pred_label == 0 and true_label == 0:\n",
        "            TN += 1\n",
        "        elif pred_label == 0 and true_label == 1:\n",
        "\n",
        "            # 18.6\n",
        "            FN += 1\n",
        "\n",
        "    # 18.7\n",
        "    total = TP+FP+TN+FN\n",
        "\n",
        "    # 18.8\n",
        "    accuracy = (TP+TN)/total if total>0 else 0\n",
        "    precision = TP/(TP+FP) if (TP+FP)>0 else 0\n",
        "    recall = TP/(TP+FN) if (TP+FN)>0 else 0\n",
        "    f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "\n",
        "# 19 MAIN\n",
        "\n",
        "#19.1 suplying the url\n",
        "csv_url = \"https://raw.githubusercontent.com/Edward-The-Great/DATAset-for-my-finals/refs/heads/main/diabetes.csv\"\n",
        "\n",
        "#19.2 Load full dataset\n",
        "df_full = load_data_from_url(csv_url)\n",
        "\n",
        "#19.3 Split train/test\n",
        "df_train, df_test = split_dataset(df_full, train_ratio=0.7)\n",
        "\n",
        "#19.4 KNN\n",
        "model_knn = KNN(df_train)\n",
        "\n",
        "# 19.5\n",
        "acc_knn_tr, prec_knn_tr, rec_knn_tr, f1_knn_tr = evaluate_knn(df_train, model_knn, k=3)\n",
        "acc_knn_te, prec_knn_te, rec_knn_te, f1_knn_te = evaluate_knn(df_test, model_knn, k=3)\n",
        "\n",
        "# 20 results!!!!!\n",
        "\n",
        "# 20.1 KNN\n",
        "print(\"                results for KNN  \")\n",
        "print(f\"Train = Accuracy={acc_knn_tr:.3f}, Precision={prec_knn_tr:.3f}, Recall={rec_knn_tr:.3f}, F1={f1_knn_tr:.3f}\")\n",
        "print(f\"Test  = Accuracy={acc_knn_te:.3f}, Precision={prec_knn_te:.3f}, Recall={rec_knn_te:.3f}, F1={f1_knn_te:.3f}\")\n",
        "\n",
        "# 20.2 Naive Bayes\n",
        "model_nb = train_naive_bayes(df_train)\n",
        "acc_nb_tr, prec_nb_tr, rec_nb_tr, f1_nb_tr = evaluate_nb(df_train, model_nb)\n",
        "acc_nb_te, prec_nb_te, rec_nb_te, f1_nb_te = evaluate_nb(df_test, model_nb)\n",
        "\n",
        "print(\"\\n            results for Naive Bayes \")\n",
        "print(f\"Train = Accuracy={acc_nb_tr:.3f}, Precision={prec_nb_tr:.3f}, Recall={rec_nb_tr:.3f}, F1={f1_nb_tr:.3f}\")\n",
        "print(f\"Test  = Accuracy={acc_nb_te:.3f}, Precision={prec_nb_te:.3f}, Recall={rec_nb_te:.3f}, F1={f1_nb_te:.3f}\")\n",
        "\n",
        "# 20.3 Decision Tree\n",
        "model_dt = train_decision_tree(df_train, max_depth=5)\n",
        "acc_dt_tr, prec_dt_tr, rec_dt_tr, f1_dt_tr = evaluate_dt(df_train, model_dt)\n",
        "acc_dt_te, prec_dt_te, rec_dt_te, f1_dt_te = evaluate_dt(df_test, model_dt)\n",
        "\n",
        "print(\"\\n           results for Decision Tree \")\n",
        "print(f\"Train = Accuracy ={acc_dt_tr:.3f}, Precision={prec_dt_tr:.3f}, Recall={rec_dt_tr:.3f}, F1={f1_dt_tr:.3f}\")\n",
        "print(f\"Test  = Accuracy ={acc_dt_te:.3f}, Precision={prec_dt_te:.3f}, Recall={rec_dt_te:.3f}, F1={f1_dt_te:.3f}\")\n",
        "\n",
        "# end of code\n"
      ]
    }
  ]
}